{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3e57b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msense2vec\u001b[39;00m \u001b[39mimport\u001b[39;00m Sense2Vec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sense2vec import Sense2Vec\n",
    "import spacy\n",
    "from sense2vec import Sense2VecComponent\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89330f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_csvs(directory):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            dataframes.append(pd.read_csv(filepath))\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "df = read_csvs(\"data\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataUnderstanding(object):\n",
    "    \"\"\" Data Understanding class\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.shape = df.shape\n",
    "        self.info = df.info\n",
    "        self.duplicates = df.duplicated().sum()\n",
    "        self.missing = df.isna().sum()\n",
    "        self.dtypes = df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91b7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the class\n",
    "understanding = dataUnderstanding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a98162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape of data\n",
    "print(f'The data has a shape of {understanding.shape[0]} rows and {understanding.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88641cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of dataframe\n",
    "understanding.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac25027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data types of the data\n",
    "understanding.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3764eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing values\n",
    "understanding.missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicates\n",
    "understanding.duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad562125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the missing value\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b66ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d1670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the questions and answers column into one column dubbed qa\n",
    "df['qa'] = df['question'].apply(lambda x:str(x)) + '' + df['answer'].apply(lambda x:str(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'qa' column from the 'final' dataframe and convert it to a list\n",
    "df_qalist = df['qa'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 elements of the final_qalist list\n",
    "df_qalist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee040fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lists where each sublist is a list of words from a document\n",
    "sentence_stream = [doc.split(\" \") for doc in df_qalist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afa978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 elements of the sentence_stream list\n",
    "sentence_stream[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aece708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk(\"C:/Users/User/Documents/Learning Python/finalproject/s2v_reddit_2015_md/s2v_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cedd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if the model is working collectly\n",
    "query = \"natural_language_processing|NOUN\"\n",
    "assert query in s2v\n",
    "vector = s2v[query]\n",
    "freq = s2v.get_freq(query)\n",
    "most_similar = s2v.most_similar(query, n=3)\n",
    "most_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590353fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_string = ' '.join(map(str, sentence_stream))\n",
    "print(sentence_string) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentence_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebe347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#from sense2vec import Sense2VecComponent\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the sense2vec component to the pipeline\n",
    "s2v = Sense2Vec().from_disk(\"C:/Users/User/Documents/Learning Python/finalproject/s2v_reddit_2015_md/s2v_old\")\n",
    "\n",
    "# Process your document\n",
    "doc = nlp(sentence_string)\n",
    "for token in doc: \n",
    "    print(token.text, token.vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f981af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qalist_vector = df_qalist.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e071c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    # Get the token's vector\n",
    "    token_vector = token.vector\n",
    "    # Compute the similarity score between the token and the mean vector\n",
    "    similarity = np.dot(doc_vector, token_vector) / (np.linalg.norm(doc_vector) * np.linalg.norm(token_vector))\n",
    "    print(token.text, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f494725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the target word\n",
    "target_word = nlp(doc)[0]\n",
    "\n",
    "# Iterate over the tokens in the document\n",
    "for token in doc:\n",
    "    # Compare the similarity of the token to the target word\n",
    "    similarity = target_word.similarity(token)\n",
    "    #if the similarity score is above a threshold, then you print the token\n",
    "    if similarity > 0.5:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be431049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f38f61e1f253a75d042c75732b1075763aaf2b6f3c885256756f2d82c9c9fe78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
